{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c3cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from numpy import cos, arctan, log, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e426afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BatchNormalization = layers.BatchNormalization\n",
    "    \n",
    "    # Parameters needed for NN architecture\n",
    "    def call(self,\n",
    "             model_input,\n",
    "             n_layers,\n",
    "             layers_dim,\n",
    "             activation,\n",
    "             BatchNormalization,\n",
    "             L1_lambda,\n",
    "             L2_lambda,\n",
    "             dropout_rate,\n",
    "             seed):\n",
    "     \n",
    "        # NN start\n",
    "        x = model_input\n",
    "        # Fix the initialization of weights\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed)\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            x = layers.Dense(layers_dim[i],\n",
    "                             activation = activation,\n",
    "                             kernel_initializer = initializer,\n",
    "                             kernel_regularizer = regularizers.L1L2(l1 = L1_lambda, l2 = L2_lambda)\n",
    "                            )(x)\n",
    "            x = layers.Dropout(rate = dropout_rate)(x)\n",
    "            if BatchNormalization:\n",
    "                x = self.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Dense(1,\n",
    "                         kernel_initializer = initializer,\n",
    "                         kernel_regularizer=regularizers.L1L2(l1 = L1_lambda, l2 = L2_lambda)\n",
    "                        )(x)\n",
    "        \n",
    "        model_output = x\n",
    "        model = keras.Model(model_input, model_output)\n",
    "        # NN end\n",
    "        \n",
    "        return model\n",
    "\n",
    "model = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2346b370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                9664      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64)               256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8)                32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,897\n",
      "Trainable params: 12,657\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Change parameters below to set NN architecture\n",
    "model_NN4_64 = model.call(\n",
    "                    # change the shape to match input shape\n",
    "                    model_input = keras.layers.Input(shape=(150, )),\n",
    "                    # number of hidden layers \n",
    "                    n_layers = 4,\n",
    "                    # dimensions of layers of length {n_layers}\n",
    "                    layers_dim = [64, 32, 16, 8],\n",
    "                    # Applied to all layers. Common activations are relu, softmax, sigmoid, tanh\n",
    "                    activation = 'sigmoid',\n",
    "                    # True or False for batch normalization. BatchNorm is applied after every layer\n",
    "                    BatchNormalization = True,\n",
    "                    # L1_regularization\n",
    "                    L1_lambda = 0,\n",
    "                    # L2_regularization\n",
    "                    L2_lambda = 0,\n",
    "                    # dropout rate\n",
    "                    dropout_rate = 0,\n",
    "                    # initialization seed\n",
    "                    seed = 120)\n",
    "model_NN4_64.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e8791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the neural network on some function\n",
    "def f(x):\n",
    "    return np.mean(x**2 + arctan(x**2))\n",
    "\n",
    "x_train = np.zeros((10000, 150, ))\n",
    "y_train = np.zeros((10000, 1, ))\n",
    "x_test = np.zeros((2000, 150, ))\n",
    "y_test = np.zeros((2000, 1, ))\n",
    "for i in range(10000):\n",
    "    x_train[i,:] = np.random.rand(150,)\n",
    "    y_train[i,:] = f(x_train[i,:]) # + np.random.rand()*0.01\n",
    "for i in range(2000):\n",
    "    x_test[i,:] = np.random.rand(150,)\n",
    "    y_test[i,:] = f(x_test[i,:]) # + np.random.rand()*0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf19907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 2s 3ms/step - loss: 0.2358 - mean_absolute_error: 0.2358 - val_loss: 0.4468 - val_mean_absolute_error: 0.4468\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.1045 - val_mean_absolute_error: 0.1045\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0228 - mean_absolute_error: 0.0228 - val_loss: 0.0585 - val_mean_absolute_error: 0.0585\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0187 - mean_absolute_error: 0.0187 - val_loss: 0.0500 - val_mean_absolute_error: 0.0500\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0164 - mean_absolute_error: 0.0164 - val_loss: 0.0341 - val_mean_absolute_error: 0.0341\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0157 - mean_absolute_error: 0.0157 - val_loss: 0.0233 - val_mean_absolute_error: 0.0233\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0155 - mean_absolute_error: 0.0155 - val_loss: 0.0241 - val_mean_absolute_error: 0.0241\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0149 - mean_absolute_error: 0.0149 - val_loss: 0.0161 - val_mean_absolute_error: 0.0161\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0145 - mean_absolute_error: 0.0145 - val_loss: 0.0241 - val_mean_absolute_error: 0.0241\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0147 - mean_absolute_error: 0.0147 - val_loss: 0.0181 - val_mean_absolute_error: 0.0181\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0146 - mean_absolute_error: 0.0146 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0144 - mean_absolute_error: 0.0144 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0137 - mean_absolute_error: 0.0137 - val_loss: 0.0206 - val_mean_absolute_error: 0.0206\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0140 - mean_absolute_error: 0.0140 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0134 - mean_absolute_error: 0.0134 - val_loss: 0.0164 - val_mean_absolute_error: 0.0164\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0134 - mean_absolute_error: 0.0134 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0134 - mean_absolute_error: 0.0134 - val_loss: 0.0152 - val_mean_absolute_error: 0.0152\n",
      "Epoch 18: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_NN4_64.compile(\n",
    "    loss = keras.losses.MeanAbsoluteError(),\n",
    "    #optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.mean_absolute_error]\n",
    ")\n",
    "\n",
    "# optional early stop\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "history = model_NN4_64.fit(x_train, y_train, batch_size=32, epochs=20, validation_split=0.2\n",
    "                    ,callbacks=[earlystop]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfe06ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 - 0s - loss: 0.0154 - mean_absolute_error: 0.0154 - 81ms/epoch - 1ms/step\n",
      "[0.015408921055495739, 0.015408921055495739]\n"
     ]
    }
   ],
   "source": [
    "test_scores = model_NN4_64.evaluate(x_test, y_test, verbose=2)\n",
    "# test_scores[0] is loss, test_score[1] is metric\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39588f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 140ms/step\n",
      "0.647414918015905 [[0.63668275]]\n"
     ]
    }
   ],
   "source": [
    "x = x_test[123,:]\n",
    "# x = np.zeros((150,))\n",
    "y = f(x)\n",
    "x = x.reshape(1, 150,)\n",
    "y_pred = model_NN4_64.predict(x)\n",
    "# Compare the real value and the predicted value\n",
    "print(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5fc60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
