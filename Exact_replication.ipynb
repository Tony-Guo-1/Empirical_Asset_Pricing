{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9416712",
   "metadata": {},
   "source": [
    "Github resource from the author: https://github.com/xiubooth/ML_Codes/tree/master/Simu_Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6937131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc # Use gc.collect() to release memory usage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed6aa5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('GKX_20201231.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e66501",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date = 19570131, 20161231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cdbba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all training data that will be used for recursive training\n",
    "data = data[(data['DATE'] >= start_date) & (data['DATE'] <= end_date)].reset_index(drop=True)\n",
    "\n",
    "# Change date format for grouping later; offsets.MonthEnd(0) means dates need not be adjusted. They were adjusted in the dataset\n",
    "data['DATE'] = pd.to_datetime(data['DATE'], format='%Y%m%d') + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Copy the data for top/bottom 1000 AND recursive training later\n",
    "# data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dbbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "characteristics = list(set(data.columns).difference({'permno','DATE','SHROUT','mve0','sic2','RET','prc'}))\n",
    "characteristics.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368e4669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prc', 'mve0', 'sic2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# BEWARE OF THE DIFFERENCE BETWEEN\n",
    "# data[ch] = data[ch].groupby(data['DATE']).transform(lambda x: x.fillna(x.median()))\n",
    "# data[ch] = data.groupby('DATE')[ch].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "for ch in characteristics:\n",
    "    data[ch] = data.groupby('DATE')[ch].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "for ch in characteristics:\n",
    "    data[ch] = data[ch].fillna(0)\n",
    "\n",
    "print(data.columns[data.isnull().sum() != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1245958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load macroeconomic predictors data. There are eight of them\n",
    "data_ma = pd.read_excel('PredictorData2022.xlsx')\n",
    "# The format of dates in macro predictors is YYYYMM instead of YYYYMMDD\n",
    "data_ma = data_ma[(data_ma['yyyymm'] >= start_date//100) & (data_ma['yyyymm'] <= end_date//100)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065c51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct predictors\n",
    "# Index, ntis, tbl, svar are given in the dataset. The remaining four are calculated using other columns\n",
    "ma_predictors = ['dp_sp','ep_sp','bm_sp','ntis','tbl','tms','dfy','svar']\n",
    "\n",
    "# data_ma['Index'] is already float64\n",
    "# data_ma['Index'] = data_ma['Index'].str.replace(',','').astype('float64')\n",
    "data_ma['dp_sp'] = data_ma['D12'] / data_ma['Index']\n",
    "data_ma['ep_sp'] = data_ma['E12'] / data_ma['Index']\n",
    "data_ma.rename({'b/m':'bm_sp'},axis=1,inplace=True)\n",
    "data_ma['tms'] = data_ma['lty'] - data_ma['tbl']\n",
    "data_ma['dfy'] = data_ma['BAA'] - data_ma['AAA']\n",
    "\n",
    "# This removes all the intermediate columns and leaves only the eight predictors, date, and risk-free rate column\n",
    "data_ma = data_ma[['yyyymm'] + ma_predictors + ['Rfree']]\n",
    "data_ma['yyyymm'] = pd.to_datetime(data_ma['yyyymm'], format='%Y%m') + pd.offsets.MonthEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ce3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picks out the top and bottom 1000 stocks per month by market value\n",
    "#data_top = data.sort_values('mvel1', ascending=False).groupby('DATE').head(1000).reset_index(drop=True)\n",
    "#data_top = data_copy.sort_values('mvel1', ascending=False).groupby('DATE').head(1000).reset_index(drop=True)\n",
    "#data_bot = data_copy.sort_values('mvel1', ascending=False).groupby('DATE').tail(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe61f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ch in characteristics:\n",
    "#     data_top[ch] = data_top.groupby('DATE')[ch].transform(lambda x: x.fillna(x.median()))\n",
    "#     data_bot[ch] = data_bot.groupby('DATE')[ch].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# print(data_top.columns[data_top.isnull().sum() != 0])\n",
    "\n",
    "# for ch in characteristics:\n",
    "#     data_top[ch] = data_top[ch].fillna(0)\n",
    "#     data_bot[ch] = data_bot[ch].fillna(0)\n",
    "\n",
    "# print(data_top.columns[data_top.isnull().sum() != 0])\n",
    "# print(data_bot.columns[data_bot.isnull().sum() != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b237896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies for SIC code\n",
    "def get_sic_dummies(data):\n",
    "    sic_dummies = pd.get_dummies(data['sic2'].fillna(999).astype(int), prefix='sic').drop('sic_999', axis=1)\n",
    "    data = pd.concat([data, sic_dummies], axis=1)\n",
    "    data.drop(['prc', 'SHROUT', 'mve0', 'sic2'], inplace=True, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f13d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_sic_dummies(data)\n",
    "#data_top = get_sic_dummies(data_top)\n",
    "#data_bot = get_sic_dummies(data_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f714599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3762139, 171)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "#print(data_top.shape)\n",
    "#print(data_bot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50498cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 1957 - 1974\n",
    "# validation 1975 - 1986\n",
    "# test 1987 - 2016\n",
    "start_val = np.datetime64('1975-01-31')\n",
    "start_test = np.datetime64('1987-01-31')\n",
    "end_test = np.datetime64('1987-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eead8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_try, data_ma_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755718cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try here first\n",
    "data_try = data[(data['DATE'] <= np.datetime64('2014-02-28')) & (data['DATE'] >= np.datetime64('2014-01-31'))]\n",
    "data_ma_long = pd.merge(data_try[['DATE']], data_ma, left_on='DATE', right_on='yyyymm', how='left').reset_index(drop=True)\n",
    "data_try = data_try.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46df4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_try.loc[:, 'RET'] = data_try.loc[:, 'RET'] - data_ma_long.loc[:, 'Rfree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24286cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_try.to_csv('raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fc in characteristics:\n",
    "    data_try[fc] = data_try.groupby('DATE')[fc].rank()\n",
    "    data_try[fc] = data_try.groupby('DATE')[fc].transform(lambda x: ((2 * (x - x.min())) / (x.max() - x.min())) - 1\n",
    "                                                     if x.max() - x.min() != 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb13cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = []\n",
    "for fc in characteristics:\n",
    "    for mp in ma_predictors:\n",
    "        data_try[fc + '*' + mp] = data_try.loc[:, fc] * data_ma_long.loc[:, mp]\n",
    "        interactions.append(fc + '*' + mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(data_try.columns).difference({'permno', 'DATE', 'RET'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e428553",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in interactions:\n",
    "    data_try[item] = data_try.groupby('DATE')[item].transform(lambda x: (x - x.mean()) / x.std() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in interactions:\n",
    "    data_try[item] = data_try.groupby('DATE')[item].transform(lambda x: ((2 * (x - x.min())) / (x.max() - x.min())) - 1\n",
    "                                                     if x.max() - x.min() != 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0bf33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_try.to_csv('transformed_data_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab794af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t_sorted = data_try.sort_values(by='mvel1', ascending=False).groupby('DATE').head(1000).reset_index(drop=True)\n",
    "x_b_sorted = data_try.sort_values(by='mvel1').groupby('DATE').head(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_b_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t_sorted.to_csv('top1k.csv')\n",
    "x_b_sorted.to_csv('bot1k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b148f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescales numbers to be in [-1, 1]\n",
    "# def rescale_group(group):\n",
    "#     if group.max() - group.min() != 0:\n",
    "#         return 2 * (group - group.min()) / (group.max() - group.min()) - 1\n",
    "#     else:\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccaf7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactions(data, data_ma, characteristics, ma_predictors):\n",
    "\n",
    "    data_ma_long = pd.merge(data[['DATE']], data_ma, left_on='DATE', right_on='yyyymm', how='left').reset_index(drop=True)\n",
    "    \n",
    "    # MUST HAVE THIS LINE OR ELSE CAN'T MULTIPLY!!!\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    # Adjust RET to excess return\n",
    "    data.loc[:, 'RET'] = data.loc[:, 'RET'] - data_ma_long.loc[:, 'Rfree']\n",
    "    \n",
    "    # Data PRE-processing: cross-sectional rank transformation\n",
    "    # data has all columns (including DATE) and all rows\n",
    "    # See page 23 of https://www.nber.org/system/files/working_papers/w24540/w24540.pdf (another Kelly et al paper)\n",
    "    for fc in characteristics:\n",
    "        data[fc] = data.groupby('DATE')[fc].rank()\n",
    "        data[fc] = data.groupby('DATE')[fc].transform(lambda x: ((2 * (x - x.min())) / (x.max() - x.min())) - 1\n",
    "                                                     if x.max() - x.min() != 0 else 0)\n",
    "    \n",
    "    interactions = []\n",
    "    for fc in characteristics:\n",
    "        for mp in ma_predictors:\n",
    "        # reference for making this less fragmented\n",
    "        # https://stackoverflow.com/questions/68292862/\n",
    "        # performancewarning-dataframe-is-highly-fragmented-this-is-usually-the-result-o\n",
    "            #col_to_add = pd.DataFrame(data.loc[:, fc] * data_ma_long.loc[:, mp], columns=[fc + '*' + mp])\n",
    "            #data = pd.concat([data, col_to_add], axis=1)\n",
    "            \n",
    "            # Maybe faster is\n",
    "            data[fc + '*' + mp] = data.loc[:, fc] * data_ma_long.loc[:, mp]\n",
    "            interactions.append(fc + '*' + mp)\n",
    "    \n",
    "    # Also scale the interactions so there are no super small or large numbers\n",
    "    for item in interactions:\n",
    "        #data[item] = data.groupby('DATE')[item].transform(lambda x: (x - x.mean()) / x.std() if x.max() - x.min() != 0 else 0)\n",
    "        data[item] = data.groupby('DATE')[item].transform(lambda x: ((2 * (x - x.min())) / (x.max() - x.min())) - 1\n",
    "                                                     if x.max() - x.min() != 0 else 0)\n",
    "        \n",
    "    # 94 (chars) * 8 (macro) + 94 (chars) + 74 (industry) = 920. This is in (fixed) random order\n",
    "    features = list(set(data.columns).difference({'permno', 'DATE', 'RET'}))\n",
    "    \n",
    "#     data[features] = MinMaxScaler((-1,1)).fit_transform(data[features])\n",
    "#     data[features] = pd.DataFrame(data, columns=features)\n",
    "    \n",
    "    # No idea why the following does not work\n",
    "    #data[characteristics] = data.groupby('DATE')[characteristics].rank()\n",
    "    #data[characteristics] = data.groupby('DATE')[characteristics].transform(rescale_group)\n",
    "    \n",
    "    # Get x and y\n",
    "    x = data[features]\n",
    "    y = pd.DataFrame(data['RET'], columns=['RET'])\n",
    "    \n",
    "    # Get top 1k\n",
    "    x_t_sorted = data.sort_values(by='mvel1', ascending=False).groupby('DATE').head(1000).reset_index(drop=True)\n",
    "    x_t = x_t_sorted[features]\n",
    "    y_t = pd.DataFrame(x_t_sorted['RET'], columns=['RET'])\n",
    "    \n",
    "    # Get bot 1k; without ascending=False, ordered in ascending order\n",
    "    x_b_sorted = data.sort_values(by='mvel1').groupby('DATE').head(1000).reset_index(drop=True)\n",
    "    x_b = x_b_sorted[features]\n",
    "    y_b = pd.DataFrame(x_b_sorted['RET'], columns=['RET'])\n",
    "    \n",
    "    print(x.shape, y.shape, x_t.shape, y_t.shape, x_b.shape, y_b.shape)\n",
    "    return x, y, x_t, y_t, x_b, y_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b5fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(479467, 920) (479467, 1) (216000, 920) (216000, 1) (216000, 920) (216000, 1)\n",
      "(773887, 920) (773887, 1) (144000, 920) (144000, 1) (144000, 920) (144000, 1)\n",
      "(83323, 920) (83323, 1) (12000, 920) (12000, 1) (12000, 920) (12000, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_train_t, y_train_t, x_train_b, y_train_b = interactions(data[data['DATE'] < start_val], \n",
    "                                                                            data_ma[data_ma['yyyymm'] < start_val],\n",
    "                                                                            characteristics, ma_predictors)\n",
    "\n",
    "x_val, y_val, x_val_t, y_val_t, x_val_b, y_val_b = interactions(data[(data['DATE'] < start_test) & (data['DATE'] >= start_val)],\n",
    "                                                                data_ma[(data_ma['yyyymm'] < start_test) & (data_ma['yyyymm'] >= start_val)],\n",
    "                                                                characteristics, ma_predictors)\n",
    "\n",
    "x_test, y_test, x_test_t, y_test_t, x_test_b, y_test_b = interactions(data[(data['DATE'] >= start_test) & (data['DATE'] <= end_test)],\n",
    "                                                                      data_ma[(data_ma['yyyymm'] >= start_test) & (data_ma['yyyymm'] <= end_test)],\n",
    "                                                                      characteristics, ma_predictors)\n",
    "gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0f87462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n",
      "Index([], dtype='object')\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check whether there are invalid data points before proceeding to train\n",
    "print(x_train.columns[x_train.isnull().sum() != 0])\n",
    "print(x_val.columns[x_val.isnull().sum() != 0])\n",
    "print(x_test.columns[x_test.isnull().sum() != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ab24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether firm characteristics are all scaled between -1 and 1\n",
    "# print(np.alltrue(abs(x_train[features])<=1))\n",
    "# print(np.alltrue(abs(x_val[features])<=1))\n",
    "# print(np.alltrue(abs(x_test[features])<=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ccca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb346fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run gets train, val, test for the first iteration\n",
    "#x_train, x_val, x_test, y_train, y_val, y_test = train_val_test_split(data)\n",
    "#x_train_t, x_val_t, x_test_t, y_train_t, y_val_t, y_test_t = train_val_test_split(data_top)\n",
    "#x_train_b, x_val_b, x_test_b, y_train_b, y_val_b, y_test_b = train_val_test_split(data_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4265bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape, y_train.shape)\n",
    "# print(x_val.shape, y_val.shape)\n",
    "# print(x_test.shape, y_test.shape)\n",
    "# print(x_train_t.shape, y_train_t.shape)\n",
    "# print(x_val_t.shape, y_val_t.shape)\n",
    "# print(x_test_t.shape, y_test_t.shape)\n",
    "# print(x_train_b.shape, y_train_b.shape)\n",
    "# print(x_val_b.shape, y_val_b.shape)\n",
    "# print(x_test_b.shape, y_test_b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457ffa7",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rf_regressor = RandomForestRegressor(n_estimators=300, max_depth=6, max_features=100).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28319330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'The total training set has MSE {mean_squared_error(y_train, rf_regressor.predict(x_train))}')\n",
    "# print(f'The total validation set has MSE {mean_squared_error(y_val, rf_regressor.predict(x_val))}')\n",
    "# print(f'The total test set has MSE {mean_squared_error(y_test, rf_regressor.predict(x_test))}')\n",
    "# print(f'The total training set has (demeaned) r^2 {r2_score(y_train, rf_regressor.predict(x_train))}')\n",
    "# print(f'The total training set has r^2 {R_oos(y_train, rf_regressor.predict(x_train))}')\n",
    "# print(f'The total validation set has r^2 {R_oos(y_val, rf_regressor.predict(x_val))}')\n",
    "# print(f'The total test set has r^2 {R_oos(y_test, rf_regressor.predict(x_test))}\\n')\n",
    "\n",
    "print(f'The top 1k training set has MSE {mean_squared_error(y_train_t, rf_regressor_t.predict(x_train_t))}')\n",
    "print(f'The top 1k validation set has MSE {mean_squared_error(y_val_t, rf_regressor_t.predict(x_val_t))}')\n",
    "print(f'The top 1k test set has MSE {mean_squared_error(y_test_t, rf_regressor_t.predict(x_test_t))}')\n",
    "print(f'The top 1k training set has (demeaned) r^2 {r2_score(y_train_t, rf_regressor_t.predict(x_train_t))}')\n",
    "print(f'The top 1k training set has r^2 {R_oos(y_train_t, rf_regressor_t.predict(x_train_t))}')\n",
    "print(f'The top 1k validation set has r^2 {R_oos(y_val_t, rf_regressor_t.predict(x_val_t))}')\n",
    "print(f'The top 1k test set has r^2 {R_oos(y_test_t, rf_regressor_t.predict(x_test_t))}\\n')\n",
    "\n",
    "print(f'The bottom 1k training set has MSE {mean_squared_error(y_train_b, rf_regressor_b.predict(x_train_b))}')\n",
    "print(f'The bottom 1k validation set has MSE {mean_squared_error(y_val_b, rf_regressor_b.predict(x_val_b))}')\n",
    "print(f'The bottom 1k test set has MSE {mean_squared_error(y_test_b, rf_regressor_b.predict(x_test_b))}')\n",
    "print(f'The bottom 1k training set has (demeaned) r^2 {r2_score(y_train_b, rf_regressor_b.predict(x_train_b))}')\n",
    "print(f'The bottom 1k training set has r^2 {R_oos(y_train_b, rf_regressor_b.predict(x_train_b))}')\n",
    "print(f'The bottom 1k validation set has r^2 {R_oos(y_val_b, rf_regressor_b.predict(x_val_b))}')\n",
    "print(f'The bottom 1k test set has r^2 {R_oos(y_test_b, rf_regressor_b.predict(x_test_b))}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df70218",
   "metadata": {},
   "source": [
    "# Apply NN and OLS_3 Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e87c21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 for OLS_3\n",
    "def R_oos(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    return 1 - (np.inner((y_true - y_pred), (y_true - y_pred))) / (np.inner(y_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd6d87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error#, r2_score\n",
    "\n",
    "# OLS with preselected size, bm, and momentum covariates\n",
    "features_3 = ['mvel1','bm','mom1m']\n",
    "OLS_3 = LinearRegression().fit(x_train[features_3], y_train)\n",
    "#OLS_3_t = LinearRegression().fit(x_train_t[features_3], y_train_t)\n",
    "#OLS_3_b = LinearRegression().fit(x_train_b[features_3], y_train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "147af529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize to record all OLS_3 results\n",
    "OLS_3_train_mse = []\n",
    "OLS_3_val_mse = []\n",
    "OLS_3_test_mse = []\n",
    "# OLS_3_train_R2_demeaned = []\n",
    "OLS_3_train_R2 = []\n",
    "OLS_3_val_R2 = []\n",
    "OLS_3_test_R2 = []\n",
    "\n",
    "#OLS_3_train_t_mse = []\n",
    "#OLS_3_val_t_mse = []\n",
    "OLS_3_test_t_mse = []\n",
    "# OLS_3_train_t_R2_demeaned = []\n",
    "# OLS_3_train_t_R2 = []\n",
    "#OLS_3_val_t_R2 = []\n",
    "OLS_3_test_t_R2 = []\n",
    "\n",
    "#OLS_3_train_b_mse = []\n",
    "#OLS_3_val_b_mse = []\n",
    "OLS_3_test_b_mse = []\n",
    "# OLS_3_train_t_R2_demeaned = []\n",
    "#OLS_3_train_b_R2 = []\n",
    "#OLS_3_val_b_R2 = []\n",
    "OLS_3_test_b_R2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f479291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total training set has MSE 0.015894677700940815\n",
      "The total validation set has MSE 0.02657529108184017\n",
      "The total test set has MSE 0.0362750727239179\n",
      "The total training set has r^2 0.003989748312870911\n",
      "The total validation set has r^2 -0.0018476187022247181\n",
      "The total test set has r^2 0.0020615522965786726\n",
      "\n",
      "The top 1k test set has MSE 0.015601747395857894\n",
      "The top 1k test set has r^2 -0.0020383520128559596\n",
      "\n",
      "The bottom 1k test set has MSE 0.08297433207552728\n",
      "The bottom 1k test set has r^2 0.0011847044500689075\n"
     ]
    }
   ],
   "source": [
    "OLS_3_train_mse.append(mean_squared_error(y_train, OLS_3.predict(x_train[features_3])))\n",
    "OLS_3_val_mse.append(mean_squared_error(y_val, OLS_3.predict(x_val[features_3])))\n",
    "OLS_3_test_mse.append(mean_squared_error(y_test, OLS_3.predict(x_test[features_3])))\n",
    "# OLS_3_train_R2_demeaned.append(r2_score(y_train, OLS_3.predict(x_train[features_3])))\n",
    "OLS_3_train_R2.append(R_oos(y_train, OLS_3.predict(x_train[features_3])))\n",
    "OLS_3_val_R2.append(R_oos(y_val, OLS_3.predict(x_val[features_3])))\n",
    "OLS_3_test_R2.append(R_oos(y_test, OLS_3.predict(x_test[features_3])))\n",
    "\n",
    "#OLS_3_train_t_mse.append(mean_squared_error(y_train_t, OLS_3.predict(x_train_t[features_3])))\n",
    "#OLS_3_val_t_mse.append(mean_squared_error(y_val_t, OLS_3.predict(x_val_t[features_3])))\n",
    "OLS_3_test_t_mse.append(mean_squared_error(y_test_t, OLS_3.predict(x_test_t[features_3])))\n",
    "# OLS_3_train_t_R2_demeaned.append(r2_score(y_train_t, OLS_3.predict(x_train_t[features_3])))\n",
    "#OLS_3_train_t_R2.append(R_oos(y_train_t, OLS_3.predict(x_train_t[features_3])))\n",
    "#OLS_3_val_t_R2.append(R_oos(y_val_t, OLS_3.predict(x_val_t[features_3])))\n",
    "OLS_3_test_t_R2.append(R_oos(y_test_t, OLS_3.predict(x_test_t[features_3])))\n",
    "\n",
    "#OLS_3_val_b_mse.append(mean_squared_error(y_val_b, OLS_3.predict(x_val_b[features_3])))\n",
    "OLS_3_test_b_mse.append(mean_squared_error(y_test_b, OLS_3.predict(x_test_b[features_3])))\n",
    "# OLS_3_train_t_R2_demeaned.append(r2_score(y_train_t, OLS_3.predict(x_train_t[features_3])))\n",
    "#OLS_3_train_t_R2.append(R_oos(y_train_t, OLS_3.predict(x_train_t[features_3])))\n",
    "#OLS_3_val_b_R2.append(R_oos(y_val_b, OLS_3.predict(x_val_b[features_3])))\n",
    "OLS_3_test_b_R2.append(R_oos(y_test_b, OLS_3.predict(x_test_b[features_3])))\n",
    "\n",
    "# OLS_3_train_t_mse.append(mean_squared_error(y_train_t, OLS_3_t.predict(x_train_t[features_3])))\n",
    "# OLS_3_val_t_mse.append(mean_squared_error(y_val_t, OLS_3_t.predict(x_val_t[features_3])))\n",
    "# OLS_3_test_t_mse.append(mean_squared_error(y_test_t, OLS_3_t.predict(x_test_t[features_3])))\n",
    "# OLS_3_train_t_R2_demeaned.append(r2_score(y_train_t, OLS_3_t.predict(x_train_t[features_3])))\n",
    "# OLS_3_train_t_R2.append(R_oos(y_train_t, OLS_3_t.predict(x_train_t[features_3])))\n",
    "# OLS_3_val_t_R2.append(R_oos(y_val_t, OLS_3_t.predict(x_val_t[features_3])))\n",
    "# OLS_3_test_t_R2.append(R_oos(y_test_t, OLS_3_t.predict(x_test_t[features_3])))\n",
    "\n",
    "print(f'The total training set has MSE {mean_squared_error(y_train, OLS_3.predict(x_train[features_3]))}')\n",
    "print(f'The total validation set has MSE {mean_squared_error(y_val, OLS_3.predict(x_val[features_3]))}')\n",
    "print(f'The total test set has MSE {mean_squared_error(y_test, OLS_3.predict(x_test[features_3]))}')\n",
    "# print(f'The total training set has (demeaned) r^2 {r2_score(y_train, OLS_3.predict(x_train[features_3]))}')\n",
    "print(f'The total training set has r^2 {R_oos(y_train, OLS_3.predict(x_train[features_3]))}')\n",
    "print(f'The total validation set has r^2 {R_oos(y_val, OLS_3.predict(x_val[features_3]))}')\n",
    "print(f'The total test set has r^2 {R_oos(y_test, OLS_3.predict(x_test[features_3]))}\\n')\n",
    "\n",
    "#print(f'The top 1k training set has MSE {mean_squared_error(y_train_t, OLS_3.predict(x_train_t[features_3]))}')\n",
    "#print(f'The top 1k validation set has MSE {mean_squared_error(y_val_t, OLS_3.predict(x_val_t[features_3]))}')\n",
    "print(f'The top 1k test set has MSE {mean_squared_error(y_test_t, OLS_3.predict(x_test_t[features_3]))}')\n",
    "#print(f'The top 1k training set has (demeaned) r^2 {r2_score(y_train_t, OLS_3.predict(x_train_t[features_3]))}')\n",
    "#print(f'The top 1k training set has r^2 {R_oos(y_train_t, OLS_3.predict(x_train_t[features_3]))}')\n",
    "#print(f'The top 1k validation set has r^2 {R_oos(y_val_t, OLS_3.predict(x_val_t[features_3]))}')\n",
    "print(f'The top 1k test set has r^2 {R_oos(y_test_t, OLS_3.predict(x_test_t[features_3]))}\\n')\n",
    "\n",
    "# print(f'The top 1k training set has MSE {mean_squared_error(y_train_t, OLS_3_t.predict(x_train_t[features_3]))}')\n",
    "# print(f'The top 1k validation set has MSE {mean_squared_error(y_val_t, OLS_3_t.predict(x_val_t[features_3]))}')\n",
    "# print(f'The top 1k test set has MSE {mean_squared_error(y_test_t, OLS_3_t.predict(x_test_t[features_3]))}')\n",
    "# print(f'The top 1k training set has (demeaned) r^2 {r2_score(y_train_t, OLS_3_t.predict(x_train_t[features_3]))}')\n",
    "# print(f'The top 1k training set has r^2 {R_oos(y_train_t, OLS_3_t.predict(x_train_t[features_3]))}')\n",
    "# print(f'The top 1k validation set has r^2 {R_oos(y_val_t, OLS_3_t.predict(x_val_t[features_3]))}')\n",
    "# print(f'The top 1k test set has r^2 {R_oos(y_test_t, OLS_3_t.predict(x_test_t[features_3]))}\\n')\n",
    "\n",
    "#print(f'The bottom 1k training set has MSE {mean_squared_error(y_train_b, OLS_3.predict(x_train_b[features_3]))}')\n",
    "#print(f'The bottom 1k validation set has MSE {mean_squared_error(y_val_b, OLS_3.predict(x_val_b[features_3]))}')\n",
    "print(f'The bottom 1k test set has MSE {mean_squared_error(y_test_b, OLS_3.predict(x_test_b[features_3]))}')\n",
    "#print(f'The bottom 1k training set has (demeaned) r^2 {r2_score(y_train_b, OLS_3.predict(x_train_b[features_3]))}')\n",
    "#print(f'The bottom 1k training set has r^2 {R_oos(y_train_b, OLS_3.predict(x_train_b[features_3]))}')\n",
    "#print(f'The bottom 1k validation set has r^2 {R_oos(y_val_b, OLS_3.predict(x_val_b[features_3]))}')\n",
    "print(f'The bottom 1k test set has r^2 {R_oos(y_test_b, OLS_3.predict(x_test_b[features_3]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c0dff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "# Import the class NN from NN_implementations\n",
    "from ipynb.fs.defs.NN_implementations import NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34dca5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4fe4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 for NN\n",
    "def R_squared(y_true, y_pred):\n",
    "    resid = tf.square(y_true - y_pred)\n",
    "    denom = tf.square(y_true)\n",
    "    return 1 - tf.divide(tf.reduce_sum(resid), tf.reduce_sum(denom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "963f0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record annual results (30, 10) = (year, model)\n",
    "loss_train = np.zeros((30, 1))\n",
    "loss_val = np.zeros((30, 1))\n",
    "loss_test = np.zeros((30, 1))\n",
    "loss_test_t = np.zeros((30, 1))\n",
    "loss_test_b = np.zeros((30, 1))\n",
    "# reg_loss_list = np.zeros((30, 10))\n",
    "# total_loss_list = np.zeros((30, 10))\n",
    "R2_train = np.zeros((30, 1))\n",
    "R2_val = np.zeros((30, 1))\n",
    "R2_test = np.zeros((30, 1))\n",
    "R2_test_t = np.zeros((30, 1))\n",
    "R2_test_b = np.zeros((30, 1))\n",
    "\n",
    "# Dictionary keys are 'year_model' (e.g. '1975_0' means training till 1975 and validation starts on 1975)\n",
    "y_train_pred_dict = {}\n",
    "y_val_pred_dict = {}\n",
    "# Dictionary keys are 'year_model' (e.g. '1987_0' means the first model testing 1987)\n",
    "y_pred_dict = {}\n",
    "y_pred_t_dict = {}\n",
    "y_pred_b_dict = {}\n",
    "\n",
    "# Record monthly results (30, 12, 10) = (year, month, model)\n",
    "loss_testM = np.zeros((30, 12))\n",
    "loss_test_tM = np.zeros((30, 12))\n",
    "loss_test_bM = np.zeros((30, 12))\n",
    "R2_testM = np.zeros((30, 12))\n",
    "R2_test_tM = np.zeros((30, 12))\n",
    "R2_test_bM = np.zeros((30, 12))\n",
    "\n",
    "# Dictionary keys are 'year_month_model' (e.g. '1987_0_3' means the fourth model testing Jan 1987)\n",
    "y_predM_dict = {}\n",
    "y_pred_tM_dict = {}\n",
    "y_pred_bM_dict = {}\n",
    "\n",
    "# Specify the hyperparameters\n",
    "L1_val = 0\n",
    "L2_val = 0.01\n",
    "dropout = 0\n",
    "lr_val = 1e-3\n",
    "bs_val = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile 10 models for ensemble later\n",
    "model_dict = {}\n",
    "for i in range(10):\n",
    "    seed_val = 120 + i\n",
    "    model_dict[str(i)] = model.call(\n",
    "                                model_input = keras.layers.Input(shape=(920, )),\n",
    "                                n_layers = 3,\n",
    "                                layers_dim = [128, 32, 8],\n",
    "                                activation = 'tanh',\n",
    "                                BatchNormalization = True,\n",
    "                                L1_lambda = L1_val,\n",
    "                                L2_lambda = L2_val,\n",
    "                                dropout_rate = dropout,\n",
    "                                seed = seed_val)\n",
    "    model_dict[str(i)].compile(\n",
    "        loss = keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(lr_val),\n",
    "        metrics=[R_squared]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62140c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved weights\n",
    "#for i in range(10):\n",
    "#    model_dict[str(i)].load_weights('model_NN3_weights_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8444c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict['9'].evaluate(x_test_t, y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49dd935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b479d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_val = np.datetime64('1975-01-31')\n",
    "# start_test = np.datetime64('1987-01-31')\n",
    "# end_test = np.datetime64('1987-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d37262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it here first\n",
    "all_months = np.arange('1987-01', '1988-02', dtype='datetime64[M]').astype('datetime64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6197a9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1987-01-01', '1987-02-01', '1987-03-01', '1987-04-01',\n",
       "       '1987-05-01', '1987-06-01', '1987-07-01', '1987-08-01',\n",
       "       '1987-09-01', '1987-10-01', '1987-11-01', '1987-12-01',\n",
       "       '1988-01-01'], dtype='datetime64[D]')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53e1a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "start_val_year = 1975\n",
    "start_test_year = 1987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ba73119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82da5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.call(\n",
    "                                model_input = keras.layers.Input(shape=(920, )),\n",
    "                                n_layers = 3,\n",
    "                                layers_dim = [128, 32, 8],\n",
    "                                activation = 'tanh',\n",
    "                                BatchNormalization = True,\n",
    "                                L1_lambda = 0,\n",
    "                                L2_lambda = 0.001,\n",
    "                                dropout_rate = 0,\n",
    "                                seed = 129)\n",
    "model.compile(\n",
    "        loss = keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        metrics=[R_squared]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa02bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "729917fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1873/1873 [==============================] - 67s 35ms/step - loss: 0.1402 - R_squared: -0.8214 - val_loss: 0.0487 - val_R_squared: -0.0691\n",
      "Epoch 2/100\n",
      "1873/1873 [==============================] - 18s 9ms/step - loss: 0.0242 - R_squared: -0.0079 - val_loss: 0.0301 - val_R_squared: -0.0243\n",
      "Epoch 3/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0177 - R_squared: 0.0037 - val_loss: 0.0287 - val_R_squared: -0.0361\n",
      "Epoch 4/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0170 - R_squared: 0.0035 - val_loss: 0.0285 - val_R_squared: -0.0499\n",
      "Epoch 5/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0168 - R_squared: 0.0025 - val_loss: 0.0273 - val_R_squared: -0.0177\n",
      "Epoch 6/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0167 - R_squared: 0.0030 - val_loss: 0.0281 - val_R_squared: -0.0361\n",
      "Epoch 7/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0165 - R_squared: 0.0046 - val_loss: 0.0277 - val_R_squared: -0.0218\n",
      "Epoch 8/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0164 - R_squared: 0.0061 - val_loss: 0.0275 - val_R_squared: -0.0168\n",
      "Epoch 9/100\n",
      "1873/1873 [==============================] - 16s 9ms/step - loss: 0.0163 - R_squared: 0.0075 - val_loss: 0.0270 - val_R_squared: -0.0077\n",
      "Epoch 10/100\n",
      "1873/1873 [==============================] - 16s 9ms/step - loss: 0.0162 - R_squared: 0.0090 - val_loss: 0.0275 - val_R_squared: -0.0213\n",
      "Epoch 11/100\n",
      "1873/1873 [==============================] - 16s 9ms/step - loss: 0.0161 - R_squared: 0.0092 - val_loss: 0.0272 - val_R_squared: -0.0098\n",
      "Epoch 12/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0160 - R_squared: 0.0097 - val_loss: 0.0268 - val_R_squared: -0.0020\n",
      "Epoch 13/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0160 - R_squared: 0.0102 - val_loss: 0.0270 - val_R_squared: -0.0095\n",
      "Epoch 14/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0159 - R_squared: 0.0107 - val_loss: 0.0268 - val_R_squared: -0.0115\n",
      "Epoch 15/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0159 - R_squared: 0.0110 - val_loss: 0.0268 - val_R_squared: -0.0044\n",
      "Epoch 16/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0159 - R_squared: 0.0112 - val_loss: 0.0266 - val_R_squared: -6.2559e-04\n",
      "Epoch 17/100\n",
      "1873/1873 [==============================] - 18s 10ms/step - loss: 0.0159 - R_squared: 0.0111 - val_loss: 0.0271 - val_R_squared: -0.0104\n",
      "Epoch 18/100\n",
      "1873/1873 [==============================] - 16s 9ms/step - loss: 0.0159 - R_squared: 0.0112 - val_loss: 0.0271 - val_R_squared: -0.0100\n",
      "Epoch 19/100\n",
      "1873/1873 [==============================] - 17s 9ms/step - loss: 0.0159 - R_squared: 0.0111 - val_loss: 0.0266 - val_R_squared: 6.2648e-04\n",
      "Epoch 20/100\n",
      "1873/1873 [==============================] - 16s 9ms/step - loss: 0.0159 - R_squared: 0.0114 - val_loss: 0.0272 - val_R_squared: -0.0212\n",
      "Epoch 21/100\n",
      "1873/1873 [==============================] - 16s 9ms/step - loss: 0.0159 - R_squared: 0.0111 - val_loss: 0.0271 - val_R_squared: -0.0183\n",
      "Epoch 21: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_data = (x_val, y_val),\n",
    "                               # change batch_size and epoch\n",
    "                               batch_size=256, epochs=100\n",
    "                               # optional early stop\n",
    "                               ,callbacks=[earlystop]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b100630",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[str(i)].save_weights(f'Resluts_log\\\\seed_129_weights_{start_test_year}_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "021fb33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25a0128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 12s 12s/step\n",
      "1/1 [==============================] - 1s 627ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_dict[f'{start_val_year}_' + str(i)] = model.predict(x_train, batch_size=x_train.shape[0])\n",
    "y_val_pred_dict[f'{start_val_year}_' + str(i)] = model.predict(x_val, batch_size=x_val.shape[0])\n",
    "y_pred_dict[f'{start_test_year}_' + str(i)] = model.predict(x_test, batch_size=x_test.shape[0])\n",
    "y_pred_t_dict[f'{start_test_year}_' + str(i)] = model.predict(x_test_t, batch_size=x_test_t.shape[0])\n",
    "y_pred_b_dict[f'{start_test_year}_' + str(i)] = model.predict(x_test_b, batch_size=x_test_b.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12b3a842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00477573],\n",
       "       [ 0.01035129],\n",
       "       [ 0.01071178],\n",
       "       ...,\n",
       "       [ 0.01140213],\n",
       "       [ 0.00664993],\n",
       "       [-0.05124054]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_dict[f'{start_test_year}_' + str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d2d1f1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0056069465"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_dict[f'{start_test_year}_' + str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "228eb172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021711268"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y_pred_dict[f'{start_test_year}_' + str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa37bbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "757"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a09ed806",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test[0, i] = mse(y_test, y_pred_dict[f'{start_test_year}_' + str(i)])\n",
    "R2_test[0, i] = R_squared(y_test, y_pred_dict[f'{start_test_year}_' + str(i)])\n",
    "loss_test_t[0, i] = mse(y_test_t, y_pred_t_dict[f'{start_test_year}_' + str(i)])\n",
    "R2_test_t[0, i] = R_squared(y_test_t, y_pred_t_dict[f'{start_test_year}_' + str(i)])\n",
    "loss_test_b[0, i] = mse(y_test_b, y_pred_b_dict[f'{start_test_year}_' + str(i)])\n",
    "R2_test_b[0, i] = R_squared(y_test_b, y_pred_b_dict[f'{start_test_year}_' + str(i)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11f8eb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03622542321681976 0.015725161880254745 0.08329074084758759 0.0034274057159334737 -0.009964680035867923 -0.0026240460291382384\n"
     ]
    }
   ],
   "source": [
    "print(loss_test[0, i], loss_test_t[0, i], loss_test_b[0, i], R2_test[0, i], R2_test_t[0, i], R2_test_b[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c54850f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train[0, i] = mse(y_train, y_train_pred_dict[f'{start_val_year}_' + str(i)])\n",
    "R2_train[0, i] = R_squared(y_train, y_train_pred_dict[f'{start_val_year}_' + str(i)])\n",
    "loss_val[0, i] = mse(y_val, y_val_pred_dict[f'{start_val_year}_' + str(i)])\n",
    "R2_val[0, i] = R_squared(y_val, y_val_pred_dict[f'{start_val_year}_' + str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d03e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015825476497411728 0.027056017890572548 0.008325995773784123 -0.019970240208824652\n"
     ]
    }
   ],
   "source": [
    "print(loss_train[0, i], loss_val[0, i], R2_train[0, i], R2_val[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3ac80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5634946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6642, 920) (6642, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get entire, top, and bottom test sets\n",
    "start_testM = all_months[j]\n",
    "end_testM = all_months[j+1]\n",
    "x_testM, y_testM, x_test_tM, y_test_tM, x_test_bM, y_test_bM = interactions(\n",
    "                                                data[(data['DATE'] >= start_testM) & (data['DATE'] <= end_testM)],\n",
    "                                                data_ma[(data_ma['yyyymm'] >= start_testM) & (data_ma['yyyymm'] <= end_testM)],\n",
    "                                                characteristics, ma_predictors)\n",
    "        \n",
    "# Find model predictions for entire/top/bottom WITHOUT batch_size\n",
    "y_predM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)] = model.predict(x_testM, batch_size=x_testM.shape[0])\n",
    "y_pred_tM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)] = model.predict(x_test_tM, batch_size=x_test_tM.shape[0])\n",
    "y_pred_bM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)] = model.predict(x_test_bM, batch_size=x_test_bM.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename for easier reference later; RET is excess return\n",
    "y_predM = pd.DataFrame(y_predM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)], columns=['Pred'])\n",
    "y_pred_tM = pd.DataFrame(y_pred_tM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)], columns=['Pred'])\n",
    "y_pred_bM = pd.DataFrame(y_pred_bM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)], columns=['Pred'])\n",
    "        \n",
    "# Record loss and R^2\n",
    "loss_testM[0, j, i] = mse(y_testM, y_predM)\n",
    "R2_testM[0, j, i] = R_oos(y_testM, y_predM)\n",
    "loss_test_tM[0, j, i] = mse(y_test_tM, y_pred_tM)\n",
    "R2_test_tM[0, j, i] = R_oos(y_test_tM, y_pred_tM)\n",
    "loss_test_bM[0, j, i] = mse(y_test_bM, y_pred_bM)\n",
    "R2_test_bM[0, j, i] = R_oos(y_test_bM, y_pred_bM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_testM[0, j, i], loss_test_tM[0, j, i], loss_test_bM[0, j, i])\n",
    "print(R2_testM[0, j, i], R2_test_tM[0, j, i], R2_test_bM[0, j, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5c1feb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_numpy = np.zeros((10, 1))\n",
    "for i in range(10):\n",
    "    std_numpy = std_numpy + np.array(std_dict[str(i)]).reshape((10, 1))\n",
    "\n",
    "std_numpy = std_numpy / 12\n",
    "std_numpy = np.sqrt(std_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "70090035",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_numpy = np.zeros((10, 1))\n",
    "for i in range(10):\n",
    "    pred_numpy = pred_numpy + np.array(pred_dict[str(i)]).reshape((10, 1))\n",
    "\n",
    "pred_numpy = pred_numpy / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8c12494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_numpy = np.zeros((10, 1))\n",
    "for i in range(10):\n",
    "    avg_numpy = avg_numpy + np.array(avg_dict[str(i)]).reshape((10, 1))\n",
    "\n",
    "avg_numpy = avg_numpy / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9aebbe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SR_numpy = avg_numpy / std_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "523b6955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02925656, -0.01808599, -0.00764438, -0.00105765, -0.00994002,\n",
       "       -0.01524728, -0.02040882, -0.01044319, -0.00666282,  0.00113671])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SR_numpy.reshape((10, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "23ed921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(np.hstack([pred_numpy, avg_numpy, std_numpy, SR_numpy]),\n",
    "                          columns=['Pred', 'Avg', 'Std', 'SR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c8f5959a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Std</th>\n",
       "      <th>SR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.043640</td>\n",
       "      <td>-0.011401</td>\n",
       "      <td>0.389702</td>\n",
       "      <td>-0.029257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021491</td>\n",
       "      <td>-0.007229</td>\n",
       "      <td>0.399688</td>\n",
       "      <td>-0.018086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.012473</td>\n",
       "      <td>-0.002955</td>\n",
       "      <td>0.386509</td>\n",
       "      <td>-0.007644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006684</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>0.381998</td>\n",
       "      <td>-0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002248</td>\n",
       "      <td>-0.003614</td>\n",
       "      <td>0.363599</td>\n",
       "      <td>-0.009940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001331</td>\n",
       "      <td>-0.005567</td>\n",
       "      <td>0.365085</td>\n",
       "      <td>-0.015247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004528</td>\n",
       "      <td>-0.006987</td>\n",
       "      <td>0.342347</td>\n",
       "      <td>-0.020409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007832</td>\n",
       "      <td>-0.003582</td>\n",
       "      <td>0.342961</td>\n",
       "      <td>-0.010443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011851</td>\n",
       "      <td>-0.002267</td>\n",
       "      <td>0.340215</td>\n",
       "      <td>-0.006663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.017688</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.300798</td>\n",
       "      <td>0.001137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pred       Avg       Std        SR\n",
       "0 -0.043640 -0.011401  0.389702 -0.029257\n",
       "1 -0.021491 -0.007229  0.399688 -0.018086\n",
       "2 -0.012473 -0.002955  0.386509 -0.007644\n",
       "3 -0.006684 -0.000404  0.381998 -0.001058\n",
       "4 -0.002248 -0.003614  0.363599 -0.009940\n",
       "5  0.001331 -0.005567  0.365085 -0.015247\n",
       "6  0.004528 -0.006987  0.342347 -0.020409\n",
       "7  0.007832 -0.003582  0.342961 -0.010443\n",
       "8  0.011851 -0.002267  0.340215 -0.006663\n",
       "9  0.017688  0.000342  0.300798  0.001137"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e6d230e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.to_csv(f'Results_log\\\\{start_test_year}_perf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0869c6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6642, 920) (6642, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "(6673, 920) (6673, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "(6740, 920) (6740, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "(6792, 920) (6792, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "(6845, 920) (6845, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "(6919, 920) (6919, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "(6996, 920) (6996, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "(7081, 920) (7081, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "(7130, 920) (7130, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "(7169, 920) (7169, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "(7172, 920) (7172, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "(7164, 920) (7164, 1) (1000, 920) (1000, 1) (1000, 920) (1000, 1)\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "    pred_dict = {}\n",
    "    std_dict = {}\n",
    "    avg_dict = {}\n",
    "    for j in range(12): # j is month\n",
    "        # Get entire, top, and bottom test sets\n",
    "        start_testM = all_months[j]\n",
    "        end_testM = all_months[j+1]\n",
    "        x_testM, y_testM, x_test_tM, y_test_tM, x_test_bM, y_test_bM = interactions(\n",
    "                                                data[(data['DATE'] >= start_testM) & (data['DATE'] <= end_testM)],\n",
    "                                                data_ma[(data_ma['yyyymm'] >= start_testM) & (data_ma['yyyymm'] <= end_testM)],\n",
    "                                                characteristics, ma_predictors)\n",
    "        \n",
    "        # Find model predictions for entire/top/bottom WITHOUT batch_size\n",
    "        y_predM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)] = model.predict(x_testM, batch_size=x_testM.shape[0])\n",
    "        y_pred_tM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)] = model.predict(x_test_tM, batch_size=x_test_tM.shape[0])\n",
    "        y_pred_bM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)] = model.predict(x_test_bM, batch_size=x_test_bM.shape[0])\n",
    "        \n",
    "        # Rename for easier reference later; RET is excess return\n",
    "        y_predM = pd.DataFrame(y_predM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)], columns=['Pred'])\n",
    "        y_pred_tM = pd.DataFrame(y_pred_tM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)], columns=['Pred'])\n",
    "        y_pred_bM = pd.DataFrame(y_pred_bM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)], columns=['Pred'])\n",
    "        \n",
    "        # Record loss and R^2\n",
    "        loss_testM[0, j] = mse(y_testM, y_predM)\n",
    "        R2_testM[0, j] = R_oos(y_testM, y_predM)\n",
    "        loss_test_tM[0, j] = mse(y_test_tM, y_pred_tM)\n",
    "        R2_test_tM[0, j] = R_oos(y_test_tM, y_pred_tM)\n",
    "        loss_test_bM[0, j] = mse(y_test_bM, y_pred_bM)\n",
    "        R2_test_bM[0, j] = R_oos(y_test_bM, y_pred_bM)  \n",
    "    \n",
    "        pred, avg, std, Sharpe = make_decile(x_testM, y_testM, y_predM)\n",
    "        pred_dict[str(j)] = pred\n",
    "        std_dict[str(j)] = std\n",
    "        avg_dict[str(j)] = avg\n",
    "        performace = pd.DataFrame(data={'Pred': pred, 'Avg': avg, 'Std': std, 'SR': Sharpe})\n",
    "        performace.to_csv(f'Results_log\\\\{start_test_year}_perf_{j}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a47fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015825476497411728 0.027056017890572548 0.03622542321681976 0.015725161880254745 0.08329074084758759\n",
      "0.008325995773784123 -0.019970240208824652 0.0034274057159334737 -0.009964680035867923 -0.0026240460291382384\n",
      "[0.05240846 0.03847449 0.03602919 0.02306702 0.02297267 0.02324396\n",
      " 0.02888217 0.02019778 0.01855616 0.09503614 0.02969969 0.04526619] [0.02269138 0.00958815 0.00834866 0.00797084 0.00595919 0.00710518\n",
      " 0.01040277 0.00598117 0.00728697 0.07478819 0.01262027 0.01595914] [0.14395161 0.09555566 0.10155001 0.06237697 0.06607016 0.06370836\n",
      " 0.07795125 0.04889692 0.04341639 0.09686999 0.06554553 0.133596  ]\n",
      "[-0.0089486  -0.01770261 -0.01146384 -0.00939329 -0.02125028 -0.02820284\n",
      " -0.03309397 -0.01553169  0.00114102  0.04251524  0.02908373  0.00522635] [ 0.0688193   0.04831142  0.00735601 -0.08368115 -0.09346453 -0.00271822\n",
      " -0.06312208 -0.02203449 -0.06090096 -0.02627831 -0.03258704  0.03242939] [-0.01938341 -0.01751421 -0.01074385 -0.01530814 -0.0109311  -0.02478008\n",
      " -0.01619507 -0.00608896 -0.01723573  0.05749444  0.03164911  0.00196761]\n"
     ]
    }
   ],
   "source": [
    "print(loss_train[0, 0], loss_val[0, 0], loss_test[0, 0], loss_test_t[0, 0], loss_test_b[0, 0])\n",
    "print(R2_train[0, 0], R2_val[0, 0], R2_test[0, 0], R2_test_t[0, 0], R2_test_b[0, 0])\n",
    "print(loss_testM[0, :], loss_test_tM[0, :], loss_test_bM[0, :])\n",
    "print(R2_testM[0, :], R2_test_tM[0, :], R2_test_bM[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed35e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decile(x_testM, y_testM, y_predM):\n",
    "    x_testM = pd.concat([x_testM, y_testM, y_predM], axis=1)\n",
    "    x_testM_grouped = x_testM.groupby(pd.qcut(x_testM.Pred, 10, labels=False))\n",
    "            \n",
    "    # https://stackoverflow.com/questions/58040767/group-pandas-dataframe-by-quantile-of-single-column/58041129#58041129\n",
    "    # decile['0'] is lowest decile, decile['9'] highest\n",
    "    decile = {}\n",
    "    for key, group in x_testM_grouped:\n",
    "        decile[str(key)] = pd.DataFrame(group).sort_values(by=['Pred'], ascending=False, ignore_index=True)\n",
    "            \n",
    "    pred = []\n",
    "    std = []\n",
    "    avg = []\n",
    "    Sharpe = []\n",
    "    for i in range(10): # 10 deciles\n",
    "        pred.append(np.mean(decile[str(i)]['Pred']))\n",
    "        avg.append(np.mean(decile[str(i)]['RET']))\n",
    "        std.append(np.std(decile[str(i)]['RET']))\n",
    "        Sharpe.append(avg[-1] / std[-1])\n",
    "            \n",
    "    return pred, avg, std, Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "# Use start years for naming keys in prediction dictionaries\n",
    "start_val_year = 1975\n",
    "start_test_year = 1987\n",
    "# Construct monthly start and end dates for test set\n",
    "start_test_ym = '1987-01'\n",
    "end_test_ym = '1988-02'\n",
    "all_months = np.arange(start_test_ym, end_test_ym, dtype='datetime64[M]').astype('datetime64[D]')\n",
    "\n",
    "for i in range(1): # i is model\n",
    "    # Housekeeping\n",
    "    gc.collect()\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model_dict[str(i)].fit(x_train, y_train, validation_data = (x_val, y_val),\n",
    "                               # change batch_size and epoch\n",
    "                               batch_size=bs_val, epochs=100\n",
    "                               # optional early stop\n",
    "                               ,callbacks=[earlystop]\n",
    "                               )\n",
    "    \n",
    "    # Save weights of all models for all years\n",
    "    # e.g. 'weights_1987_0' means the first model weights when the test set starts on 1987, so training ends on 1987-13=1974\n",
    "    model_dict[str(i)].save_weights(f'Results_log\\\\seed129_{start_test_year}_' + str(i) + '.h5')\n",
    "    \n",
    "    # Find model predictions for entire/top/bottom WITHOUT batch_size\n",
    "    y_train_pred_dict[f'{start_val_year}_' + str(i)] = model_dict[str(i)].predict(x_train, batch_size=x_train.shape[0])\n",
    "    y_val_pred_dict[f'{start_val_year}_' + str(i)] = model_dict[str(i)].predict(x_val, batch_size=x_val.shape[0])\n",
    "    y_pred_dict[f'{start_test_year}_' + str(i)] = model_dict[str(i)].predict(x_test, batch_size=x_test.shape[0])\n",
    "    y_pred_t_dict[f'{start_test_year}_' + str(i)] = model_dict[str(i)].predict(x_test_t, batch_size=x_test_t.shape[0])\n",
    "    y_pred_b_dict[f'{start_test_year}_' + str(i)] = model_dict[str(i)].predict(x_test_b, batch_size=x_test_b.shape[0])\n",
    "    predY, avgY, stdY, SharpeY = make_decile(x_test, y_test, pd.DataFrame(y_pred_dict[f'{start_test_year}_' + str(i)]), columns=['Pred'])\n",
    "    performace = pd.DataFrame(data={'Pred': pred, 'Avg': avg, 'Std': std, 'SR': Sharpe})\n",
    "    performace.to_csv(f'Results_log\\\\{start_test_year}_perf.csv')\n",
    "        \n",
    "    # Record loss and R^2; train and validation\n",
    "    loss_train[0, i] = mse(y_train, y_train_pred_dict[f'{start_val_year}_' + str(i)])\n",
    "    R2_train[0, i] = R_squared(y_train, y_train_pred_dict[f'{start_val_year}_' + str(i)])\n",
    "    loss_val[0, i] = mse(y_val, y_val_pred_dict[f'{start_val_year}_' + str(i)])\n",
    "    R2_val[0, i] = R_squared(y_val, y_val_pred_dict[f'{start_val_year}_' + str(i)])\n",
    "    # test, top/bottom test\n",
    "    loss_test[0, i] = mse(y_test, y_pred_dict[f'{start_test_year}_' + str(i)])\n",
    "    R2_test[0, i] = R_squared(y_test, y_pred_dict[f'{start_test_year}_' + str(i)])\n",
    "    loss_test_t[0, i] = mse(y_test_t, y_pred_t_dict[f'{start_test_year}_' + str(i)])\n",
    "    R2_test_t[0, i] = R_squared(y_test_t, y_pred_t_dict[f'{start_test_year}_' + str(i)])\n",
    "    loss_test_b[0, i] = mse(y_test_b, y_pred_b_dict[f'{start_test_year}_' + str(i)])\n",
    "    R2_test_b[0, i] = R_squared(y_test_b, y_pred_b_dict[f'{start_test_year}_' + str(i)]) \n",
    "    \n",
    "    decile = {}\n",
    "    std = {}\n",
    "    avg = {}\n",
    "    Sharpe = {}\n",
    "    for j in range(12): # j is month\n",
    "        # Get entire, top, and bottom test sets\n",
    "        start_testM = all_months[j]\n",
    "        end_testM = all_months[j+1]\n",
    "        x_testM, y_testM, x_test_tM, y_test_tM, x_test_bM, y_test_bM = interactions(\n",
    "                                                data[(data['DATE'] >= start_testM) & (data['DATE'] <= end_testM)],\n",
    "                                                data_ma[(data_ma['yyyymm'] >= start_testM) & (data_ma['yyyymm'] <= end_testM)],\n",
    "                                                characteristics, ma_predictors)\n",
    "        \n",
    "        # Find model predictions for entire/top/bottom WITHOUT batch_size\n",
    "        y_predM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)] = model_dict[str(i)].predict(x_testM, batch_size=x_testM.shape[0])\n",
    "        y_pred_tM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)] = model_dict[str(i)].predict(x_test_tM, batch_size=x_test_tM.shape[0])\n",
    "        y_pred_bM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)] = model_dict[str(i)].predict(x_test_bM, batch_size=x_test_bM.shape[0])\n",
    "        \n",
    "        # Rename for easier reference later; RET is excess return\n",
    "        y_predM = pd.DataFrame(y_predM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)], columns=['Pred'])\n",
    "        y_pred_tM = pd.DataFrame(y_pred_tM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)], columns=['Pred'])\n",
    "        y_pred_bM = pd.DataFrame(y_pred_bM_dict[f'{start_test_year}_' + str(j) + '_' + str(i)], columns=['Pred'])\n",
    "        \n",
    "        # Record loss and R^2\n",
    "        loss_testM[0, j, i] = mse(y_testM, y_predM)\n",
    "        R2_testM[0, j, i] = R_squared(y_testM, y_predM)\n",
    "        loss_test_tM[0, j, i] = mse(y_test_tM, y_pred_tM)\n",
    "        R2_test_tM[0, j, i] = R_squared(y_test_tM, y_pred_tM)\n",
    "        loss_test_bM[0, j, i] = mse(y_test_bM, y_pred_bM)\n",
    "        R2_test_bM[0, j, i] = R_squared(y_test_bM, y_pred_bM)  \n",
    "    \n",
    "        pred, std, avg, Sharpe = make_decile(x_testM, y_testM, y_predM)\n",
    "        performace = pd.DataFrame(data={'Pred': pred, 'Avg': avg, 'Std': std, 'SR': Sharpe})\n",
    "        performace.to_csv(f'Results_log\\\\{start_test_year}_perf_{j}.csv')\n",
    "\n",
    "print(loss_train[0, 0], loss_val[0, 0], loss_test[0, 0], loss_test_t[0, 0], loss_test_b[0, 0])\n",
    "print(R2_train[0, 0], R2_val[0, 0], R2_test[0, 0], R2_test_t[0, 0], R2_test_b[0, 0])\n",
    "print(loss_testM[0, :], loss_test_tM[0, :], loss_test_bM[0, :])\n",
    "print(R2_testM[0, :], R2_test_tM[0, :], R2_test_bM[0, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68b3ee",
   "metadata": {},
   "source": [
    "# Recursively do OLS_3 and refit NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec62049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime64(date_digits):\n",
    "    date_str = str(date_digits)\n",
    "    date_str = date_str[0:4] + '-' + date_str[4:6] + '-' + date_str[6:8]\n",
    "    return date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed91e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_val = np.datetime64('1975-01-31')\n",
    "# start_test = np.datetime64('1987-01-31')\n",
    "# end_test = np.datetime64('1987-12-31')\n",
    "start_test_year = 1987\n",
    "end_test_year = 1988\n",
    "start_val_numer = 19750131\n",
    "start_test_numer = 19870131\n",
    "end_test_numer = 19871231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92abdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the sizes of training, validation, test sets\n",
    "train_shape = [0]*30\n",
    "val_shape = [0]*30\n",
    "test_shape = [0]*30\n",
    "train_shape[0] = 479467\n",
    "val_shape[0] = 773887\n",
    "test_shape[0] = 83323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb115130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are in total 30 OOS years to test (1987-2016)\n",
    "# Train every year, but test every month and every year\n",
    "for year in range(1, 30):\n",
    "    \n",
    "    gc.collect()\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    # Set the correct dates\n",
    "    start_val_prev_numer = start_val_numer\n",
    "    start_test_prev_numer = start_test_numer\n",
    "    end_test_prev_numer = end_test_numer\n",
    "    start_val_numer = start_val_numer + 10000\n",
    "    start_test_numer = start_test_numer + 10000\n",
    "    end_test_numer = end_test_numer + 10000\n",
    "    start_val_prev = get_datetime64(start_val_prev_numer)\n",
    "    start_test_prev = get_datetime64(start_test_prev_numer)\n",
    "    end_test_prev = get_datetime64(end_test_prev_numer)\n",
    "    start_val = get_datetime64(start_val_numer)\n",
    "    start_test = get_datetime64(start_test_numer)\n",
    "    end_test = get_datetime64(end_test_numer)\n",
    "    print(start_val_prev, start_test_prev, end_test_prev, start_val, start_test, end_test)\n",
    "    \n",
    "    # Add one more year to training\n",
    "    x_train_add, y_train_add, _, _, _, _ = interactions(data[(data['DATE'] < start_val) & (data['DATE'] >= start_val_prev)],\n",
    "                                           data_ma[(data_ma['yyyymm'] < start_val) & (data_ma['yyyymm'] >= start_val_prev)],\n",
    "                                           characteristics, ma_predictors)\n",
    "    x_train = pd.concat([x_train, x_train_add], ignore_index=True)\n",
    "    y_train = pd.concat([y_train, y_train_add], ignore_index=True)\n",
    "    train_shape[year] = x_train.shape[0]\n",
    "    \n",
    "    # Since x,y_val has no more date inside, we will just get them again\n",
    "    x_val, y_val, _, _, _, _ = interactions(data[(data['DATE'] < start_test) & (data['DATE'] >= start_val)],\n",
    "                                                                    data_ma[(data_ma['yyyymm'] < start_test) & (data_ma['yyyymm'] >= start_val)],\n",
    "                                                                    characteristics, ma_predictors)\n",
    "    val_shape[year] = x_val.shape[0]\n",
    "\n",
    "    # Change the test set to the next year\n",
    "    x_test, y_test, x_test_t, y_test_t, x_test_b, y_test_b = interactions(data[(data['DATE'] >= start_test) & (data['DATE'] <= end_test)],\n",
    "                                                                          data_ma[(data_ma['yyyymm'] >= start_test) & (data_ma['yyyymm'] <= end_test)],\n",
    "                                                                          characteristics, ma_predictors)\n",
    "    test_shape[year] = x_test.shape[0]\n",
    "    \n",
    "    # Do OLS_3 first\n",
    "    # Replace the OLS model every iteration (could put in dictionary if want to save all 30 of them)\n",
    "    OLS_3 = LinearRegression().fit(x_train[features_3], y_train)\n",
    "    \n",
    "    OLS_3_train_mse.append(mean_squared_error(y_train, OLS_3.predict(x_train[features_3])))\n",
    "    OLS_3_val_mse.append(mean_squared_error(y_val, OLS_3.predict(x_val[features_3])))\n",
    "    OLS_3_test_mse.append(mean_squared_error(y_test, OLS_3.predict(x_test[features_3])))\n",
    "    OLS_3_train_R2.append(R_oos(y_train, OLS_3.predict(x_train[features_3])))\n",
    "    OLS_3_val_R2.append(R_oos(y_val, OLS_3.predict(x_val[features_3])))\n",
    "    OLS_3_test_R2.append(R_oos(y_test, OLS_3.predict(x_test[features_3])))\n",
    "    \n",
    "    #OLS_3_val_t_mse.append(mean_squared_error(y_val_t, OLS_3.predict(x_val_t[features_3])))\n",
    "    OLS_3_test_t_mse.append(mean_squared_error(y_test_t, OLS_3.predict(x_test_t[features_3])))\n",
    "    #OLS_3_val_b_mse.append(mean_squared_error(y_val_b, OLS_3.predict(x_val_b[features_3])))\n",
    "    OLS_3_test_b_mse.append(mean_squared_error(y_test_b, OLS_3.predict(x_test_b[features_3])))\n",
    "    #OLS_3_train_t_R2_demeaned.append(r2_score(y_train_t, OLS_3.predict(x_train_t[features_3])))\n",
    "    #OLS_3_train_t_R2.append(R_oos(y_train_t, OLS_3.predict(x_train_t[features_3])))\n",
    "    #OLS_3_val_t_R2.append(R_oos(y_val_t, OLS_3.predict(x_val_t[features_3])))\n",
    "    OLS_3_test_t_R2.append(R_oos(y_test_t, OLS_3.predict(x_test_t[features_3])))\n",
    "    #OLS_3_val_b_R2.append(R_oos(y_val_b, OLS_3.predict(x_val_b[features_3])))\n",
    "    OLS_3_test_b_R2.append(R_oos(y_test_b, OLS_3.predict(x_test_b[features_3])))\n",
    "    print(OLS_3_train_R2[-1], OLS_3_val_R2[-1],  OLS_3_val_t_R2[-1], OLS_3_val_b_R2[-1], OLS_3_test_R2[-1], OLS_3_test_t_R2[-1], \n",
    "         OLS_3_test_b_R2[-1])\n",
    "    \n",
    "    for i in range(10):\n",
    "        # Housekeeping\n",
    "        keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "        # This refits the model used before\n",
    "        history = model_dict[str(i)].fit(x_train, y_train, validation_data = (x_val, y_val),\n",
    "                               # change batch_size and epoch\n",
    "                               batch_size=bs_val, epochs=100\n",
    "                               # optional early stop\n",
    "                               ,callbacks=[earlystop]\n",
    "                               )\n",
    "    \n",
    "        loss_val_t[year, i], R2_val_t[year, i] = model_dict[str(i)].evaluate(x_val_t, y_val_t, batch_size=bs_val)\n",
    "        loss_val_b[year, i], R2_val_b[year, i] = model_dict[str(i)].evaluate(x_val_b, y_val_b, batch_size=bs_val)\n",
    "        loss_test[year, i], R2_test[year, i] = model_dict[str(i)].evaluate(x_test, y_test, batch_size=bs_val)\n",
    "        loss_test_t[year, i], R2_test_t[year, i] = model_dict[str(i)].evaluate(x_test_t, y_test_t, batch_size=bs_val)\n",
    "        loss_test_b[year, i], R2_test_b[year, i] = model_dict[str(i)].evaluate(x_test_b, y_test_b, batch_size=bs_val)\n",
    "\n",
    "    #print(loss_val_t[year, :], loss_val_b[year, :], loss_test[year, :], loss_test_t[year, :], loss_test_b[year, :])\n",
    "    print(R2_val_t[year, :], R2_val_b[year, :], R2_test[year, :], R2_test_t[year, :], R2_test_b[year, :])    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some references for model.evaluate()\n",
    "# https://stackoverflow.com/questions/50723287/meaning-of-batch-size-in-model-evaluate (floating point error)\n",
    "# https://stackoverflow.com/questions/49359489/how-are-metrics-computed-in-keras (val metric)\n",
    "\n",
    "print(OLS_3_train_R2)\n",
    "print(OLS_3_val_R2)\n",
    "print(OLS_3_val_t_R2)\n",
    "print(OLS_3_val_b_R2)\n",
    "print(OLS_3_test_R2)\n",
    "print(OLS_3_test_t_R2)\n",
    "print(OLS_3_test_b_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755bbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R2_val_t[0:12, :])\n",
    "print(R2_val_b[0:12, :])\n",
    "print(R2_test[0:12, :])\n",
    "print(R2_test_t[0:12, :])\n",
    "print(R2_test_b[0:12, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    model_dict[str(i)].save_weights(f'model_weights_{i}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['9'].evaluate(x_val_t, y_val_t, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(R2_test[0:12,:], axis=1) - np.array(OLS_3_test_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b8733",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(R2_test_t[0:12,:], axis=1) - np.array(OLS_3_test_t_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726149bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(R2_test_b[0:12,:], axis=1) - np.array(OLS_3_test_b_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e62d2",
   "metadata": {},
   "source": [
    "# Other NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble method NN3\n",
    "loss_list_3 = []\n",
    "R2_list_3 = []\n",
    "for i in range(10):\n",
    "    seed_val = 120 + i\n",
    "    model_NN3_dft = model.call(\n",
    "                    model_input = keras.layers.Input(shape=(920, )),\n",
    "                    n_layers = 3,\n",
    "                    activation = 'relu',\n",
    "                    BatchNormalization = True,\n",
    "                    first_layer_dim = 32,\n",
    "                    L1_lambda = 0.5,\n",
    "                    seed = seed_val)\n",
    "    \n",
    "    model_NN3_dft.compile(\n",
    "    loss = keras.losses.MeanSquaredError(),\n",
    "    # Specify the learning rate\n",
    "    optimizer=keras.optimizers.Adam(2e-4),\n",
    "    metrics=[R_squared]\n",
    "    )\n",
    "\n",
    "    # optional early stop\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    history = model_NN3_dft.fit(x_train, y_train, validation_data = (x_val, y_val),\n",
    "                           # change batch_size and epoch\n",
    "                           batch_size=512, epochs=100\n",
    "                           # optional early stop\n",
    "                           ,callbacks=[earlystop]\n",
    "                           )\n",
    "    loss_test, R2_test = model_NN3_dft.evaluate(x_test, y_test)\n",
    "    loss_list_3.append(loss_test)\n",
    "    R2_list_3.append(R2_test)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97032058",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_list_3)\n",
    "print(R2_list_3)\n",
    "print(np.mean(loss_list_3))\n",
    "print(np.mean(R2_list_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890cd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble method NN2\n",
    "loss_list_2 = []\n",
    "R2_list_2 = []\n",
    "for i in range(10):\n",
    "    seed_val = 120 + i\n",
    "    model_NN2_dft = model.call(\n",
    "                    model_input = keras.layers.Input(shape=(920, )),\n",
    "                    n_layers = 2,\n",
    "                    activation = 'relu',\n",
    "                    BatchNormalization = True,\n",
    "                    first_layer_dim = 32,\n",
    "                    L1_lambda = 0.5,\n",
    "                    seed = seed_val)\n",
    "    \n",
    "    model_NN2_dft.compile(\n",
    "    loss = keras.losses.MeanSquaredError(),\n",
    "    # Specify the learning rate\n",
    "    optimizer=keras.optimizers.Adam(2e-4),\n",
    "    metrics=[R_squared]\n",
    "    )\n",
    "\n",
    "    # optional early stop\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    history = model_NN2_dft.fit(x_train, y_train, validation_data = (x_val, y_val),\n",
    "                           # change batch_size and epoch\n",
    "                           batch_size=512, epochs=100\n",
    "                           # optional early stop\n",
    "                           ,callbacks=[earlystop]\n",
    "                           )\n",
    "    loss_test, R2_test = model_NN2_dft.evaluate(x_test, y_test)\n",
    "    loss_list_2.append(loss_test)\n",
    "    R2_list_2.append(R2_test)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92143c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_list_2)\n",
    "print(R2_list_2)\n",
    "print(np.mean(loss_list_2))\n",
    "print(np.mean(R2_list_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble method NN5\n",
    "loss_list_5 = []\n",
    "R2_list_5 = []\n",
    "for i in range(10):\n",
    "    seed_val = 120 + i\n",
    "    model_NN5_dft = model.call(\n",
    "                    model_input = keras.layers.Input(shape=(920, )),\n",
    "                    n_layers = 5,\n",
    "                    activation = 'relu',\n",
    "                    BatchNormalization = True,\n",
    "                    first_layer_dim = 32,\n",
    "                    L1_lambda = 0.5,\n",
    "                    seed = seed_val)\n",
    "    \n",
    "    model_NN5_dft.compile(\n",
    "    loss = keras.losses.MeanSquaredError(),\n",
    "    # Specify the learning rate\n",
    "    optimizer=keras.optimizers.Adam(2e-4),\n",
    "    metrics=[R_squared]\n",
    "    )\n",
    "\n",
    "    # optional early stop\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    history = model_NN5_dft.fit(x_train, y_train, validation_data = (x_val, y_val),\n",
    "                           # change batch_size and epoch\n",
    "                           batch_size=512, epochs=100\n",
    "                           # optional early stop\n",
    "                           ,callbacks=[earlystop]\n",
    "                           )\n",
    "    loss_test, R2_test = model_NN5_dft.evaluate(x_test, y_test)\n",
    "    loss_list_5.append(loss_test)\n",
    "    R2_list_5.append(R2_test)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbbd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_list_5)\n",
    "print(R2_list_5)\n",
    "print(np.mean(loss_list_5))\n",
    "print(np.mean(R2_list_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5197b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d99f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN4 = model.call(\n",
    "                    # change the shape to match input shape\n",
    "                    model_input = keras.layers.Input(shape=(920, )),\n",
    "                    # number of hidden layers \n",
    "                    n_layers = 4,\n",
    "                    # Applied to all layers. Common activations are relu, softmax, sigmoid, tanh\n",
    "                    activation = 'relu',\n",
    "                    # True or False for batch normalization. BatchNorm is applied after every layer\n",
    "                    BatchNormalization = True,\n",
    "                    # number of neurons in the first layer. Assume every subsequent layer has half as many neurons as the previous layer\n",
    "                    first_layer_dim = 32,\n",
    "                    # L2_lambda is the parameter for L2_regularization. Set to 0 for no regularization, default is 0.01\n",
    "                    L2_lambda = 1e-4)\n",
    "model_NN4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e994d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN4.compile(\n",
    "    loss = keras.losses.MeanSquaredError(),\n",
    "    # Specify the learning rate\n",
    "    optimizer=keras.optimizers.Adam(0.01),\n",
    "    metrics=[R_squared]\n",
    ")\n",
    "\n",
    "# optional early stop\n",
    "#earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "history = model_NN4.fit(x_train, y_train, validation_data = (x_val, y_val),\n",
    "                           # change batch_size and epoch\n",
    "                           batch_size=10000, epochs=100\n",
    "                           # optional early stop\n",
    "                           #,callbacks=[earlystop]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN4.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020a4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
